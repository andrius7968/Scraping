import html
import requests  #sirve para hacer solicitudes HTTP
import csv  # sirve para escribir archivos CSV
import re # busca patrones en el texto y extraer informaci√≥n espec√≠fica y reducir espacios extra
import time
from bs4 import BeautifulSoup
import os
import unicodedata

from conexion_sql import guardar_en_mysql

URL = ""

#URL = "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0001671425" #Profe Saray
#URL = "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0001685519" #Jaime Blanco Lopez
#URL = "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0000113761" #Addriana
#URL = "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0001740020" #Edna Conde
#URL = "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0001473049" #Ana Cristina Zuniga
#URL = "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0000003029" #Javier Cordoba
#URL = "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0001006690" #Jhon ni√±o
#URL = "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0000674400&mostrar=produccion" #Walter arboleda
#URL = "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0001413648" #Wilson Arana

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/121.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "es-CO,es;q=0.9",
    "Referer": "https://scienti.minciencias.gov.co/",
    "Connection": "keep-alive"
}
# Nota: El sitio de CVLAC puede bloquear solicitudes si detecta tr√°fico sospechoso.
# Por ende se usa el headers para simular un navegador real
# y se implementan reintentos con espera entre ellos.

archivo_csv = "cv_datos_generales.csv"

# Borra el archivo si ya existe
if os.path.exists(archivo_csv):
    os.remove(archivo_csv)

def quitar_tildes(texto):
    if not texto:
        return ""
    texto = unicodedata.normalize("NFD", texto)
    texto = texto.encode("ascii", "ignore").decode("utf-8")
    return texto
def limpiar(texto):
    if not texto:
        return ""
    texto = texto.replace("\xa0", " ")
    texto = re.sub(r"\s+", " ", texto)
    return texto.strip()
# La funci√≥n limpiar se encarga de eliminar espacios extra y caracteres
# no deseados del texto extra√≠do.
def limpiar_titulo(titulo):
    """
    Quita tildes, espacios, comillas y reemplaza comas internas por punto y coma.
    """
    if not titulo:
        return ""
    titulo = quitar_tildes(titulo)        # Quita tildes
    titulo = titulo.replace('"', '')      # Quita comillas dobles
    titulo = titulo.replace("'", '')      # Quita comillas simples
    titulo = titulo.replace(",", ";")     # Reemplaza comas internas
    titulo = titulo.strip()               # Quita espacios al inicio y al final
    return titulo
def limpiar_categoria(texto):
    if not texto:
        return ""

    match = re.search(
        r"(Investigador\s+(Junior|Asociado|Senior))",
        texto,
        re.IGNORECASE
    )

    if match:
        return match.group(1).title()

    return texto.strip()

def limpiar_tipo_trabajo(texto):
    if not texto:
        return ""

    texto = limpiar(texto)

    # Nos quedamos SOLO con lo que est√° despu√©s del "de"
    if "-" in texto:
        texto = texto.split("de", 1)[1].strip()

    # Aseguramos formato uniforme
    texto = texto.lower()

    # Construimos el texto final
    return f"Trabajo dirigido de {texto}"

def limpiar_tipo_consultoria(texto):
    texto = limpiar(texto)

    if "-" in texto:
        texto = texto.split("-", 1)[1].strip()

    return texto


def obtener_html():
    session = requests.Session()
    session.headers.update(HEADERS)

    for intento in range(5):
        print(f"Intento {intento + 1} de conexi√≥n...")
        response = session.get(URL, timeout=30)

        if response.status_code == 200:

            # üî• Decodificar manualmente desde bytes
            html = response.content.decode("latin-1")

            return html

        print(f"Servidor respondi√≥ {response.status_code}, esperando...")
        time.sleep(5)

    raise Exception("No fue posible acceder a CVLAC (bloqueo del servidor)")
# La funci√≥n obtener_html intenta obtener el HTML de la p√°gina con reintentos y espera

#================================================
# EXTRAER DATOS GENERALES
#================================================
def extraer_datos_generales(soup):
    datos = {
        "categoria": "No categorizado",  # Valor por defecto
        "nombre": "",
        "sexo": ""
    }

    # 1Ô∏è‚É£ Buscar el ancla
    anchor = soup.find("a", {"name": "datos_generales"})
    if not anchor:
        print("No se encontr√≥ el ancla datos_generales")
        return datos

    # 2Ô∏è‚É£ La tabla est√° justo despu√©s del ancla
    tabla = anchor.find_next("table")
    if not tabla:
        print("No se encontr√≥ la tabla de datos generales")
        return datos

    # 3Ô∏è‚É£ Recorrer filas
    for fila in tabla.find_all("tr"):
        columnas = fila.find_all("td")
        if len(columnas) == 2:
            campo = limpiar(columnas[0].get_text())
            valor = limpiar(columnas[1].get_text())

            if campo == "Categor√≠a":
                categoria_limpia = limpiar_categoria(valor)
                if categoria_limpia:  # Si existe una categor√≠a v√°lida
                    datos["categoria"] = categoria_limpia
                # Si no, se queda "No categorizado"
            elif campo == "Nombre":
                datos["nombre"] = valor
            elif campo == "Sexo":
                datos["sexo"] = valor

    return datos


#================================================
# EXTRAER √öLTIMA FORMACI√ìN ACAD√âMICA
#================================================
def extraer_ultima_formacion_academica(soup):
    formacion = {
        "UltimaFormacionAcademica": ""
    }

    # 1Ô∏è‚É£ Buscar el ancla
    anchor = soup.find("a", {"name": "formacion_acad"})
    if not anchor:
        print("No se encontr√≥ el ancla formacion_acad")
        return formacion

    # 2Ô∏è‚É£ Buscar la tabla de formaci√≥n acad√©mica
    tabla = anchor.find_next("table")
    if not tabla:
        print("No se encontr√≥ la tabla de formaci√≥n acad√©mica")
        return formacion

    # 3Ô∏è‚É£ Buscar el primer <b> (nivel acad√©mico)
    bold = tabla.find("b")
    if not bold:
        print("No se encontr√≥ el nivel acad√©mico")
        return formacion

    texto_nivel = bold.get_text(strip=True)

    # 4Ô∏è‚É£ Quedarse solo con "Maestr√≠a"
    formacion["UltimaFormacionAcademica"] = texto_nivel.split("/")[0].strip()

    return formacion

#================================================
# EXTRAER TRABAJOS DIRIGIDOS
#================================================
def extraer_trabajos_dirigidos(soup):
    

    resultados = []

    anchor = soup.find("a", {"name": "trabajos_dirigi"})
    if not anchor:
        print("No se encontr√≥ la secci√≥n trabajos dirigidos")
        return resultados

    contenedor = anchor.find_parent("td")

    tipo_trabajo_actual = ""

    # Recorremos en orden todo lo que hay dentro del contenedor
    for elemento in contenedor.find_all(["b", "blockquote"], recursive=True):

        # 1Ô∏è‚É£ Si es un <b>, actualizamos el tipo de trabajo
        if elemento.name == "b":
            texto_b = limpiar(elemento.get_text())

            if "trabajos dirigidos/tutor√≠as" in texto_b.lower():
                tipo_trabajo_actual = limpiar_tipo_trabajo(texto_b)
            else:
                tipo_trabajo_actual = ""

        # 2Ô∏è‚É£ Si es un <blockquote>, es un trabajo
        elif elemento.name == "blockquote" and tipo_trabajo_actual:
            texto = limpiar(elemento.get_text(" "))

            # üîπ Separar por la PRIMERA coma (autor / t√≠tulo)
            partes = texto.split(",", 1)

            if len(partes) < 2:
                continue  # si no hay coma, ignorar

            texto_sin_autor = partes[1].strip()

            # üîπ A√±o
            a√±o_match = re.search(r"\b(20\d{2})\b", texto_sin_autor)
            a√±o = a√±o_match.group(1) if a√±o_match else ""

            # üîπ Cortar antes de "Estado:"
            estado_match = re.search(r"^(.*?)(?=Estado:)", texto_sin_autor, re.IGNORECASE)

            if estado_match:
                titulo = estado_match.group(1).strip(" ,")
            else:
                titulo = texto_sin_autor.strip(" ,")

            resultados.append({
                "NodoHijo": tipo_trabajo_actual,
                "Titulo_proyecto": titulo,
                "a√±o": a√±o
            })
    print(f"‚úÖ Total TRABAJOS DIRIGIDOS: {len(resultados)}")
    return resultados

#================================================
# EXTRAER CONSULTOR√çAS
#================================================
def extraer_consultorias(soup):
    

    resultados = []

    # 1Ô∏è‚É£ Buscar el encabezado exacto
    h3 = soup.find("h3", id="trabajos_tec")
    if not h3:
        print("‚ö†Ô∏è No se encontr√≥ el h3 de Consultor√≠as")
        return resultados

    # 2Ô∏è‚É£ Buscar el <b> del tipo de consultor√≠a
    tipo_b = h3.find_next("b")
    if not tipo_b:
        print("‚ö†Ô∏è No se encontr√≥ el tipo de consultor√≠a")
        return resultados

    tipo_actual = limpiar_tipo_consultoria(tipo_b.get_text())

    # 3Ô∏è‚É£ Recorrer TODOS los blockquote siguientes
    for block in tipo_b.find_all_next("blockquote"):
        texto = limpiar(block.get_text(" "))

        # Cortar cuando aparezca otra secci√≥n
        if block.find_previous("h3") != h3:
            break

        # üü¢ A√±o: buscar el a√±o que aparece despu√©s de "En: <pa√≠s>", ignorando comas/espacios extra
        anio_match = re.search(r"En:\s*[A-Za-z\s]+(?:,\s*)*,\s*(\d{4})", texto)
        anio = anio_match.group(1) if anio_match else ""

        # üü¢ T√≠tulo: todo hasta "Nombre comercial"
        hasta_nombre = re.search(r"^(.*?)(?=Nombre comercial)", texto, re.IGNORECASE)
        titulo = ""
        if hasta_nombre:
            texto_hasta_nombre = hasta_nombre.group(1).strip()

            # Buscar la √∫ltima coma que tenga may√∫scula a la derecha
            match_coma = list(re.finditer(r",\s*(?=[A-Z])", texto_hasta_nombre))
            if match_coma:
                ultima_coma = match_coma[-1].end()  # posici√≥n final de la coma
                titulo = texto_hasta_nombre[ultima_coma:].strip(" ,")
            else:
                titulo = texto_hasta_nombre.strip(" ,")  # si no hay, tomar todo

        resultados.append({
            "NodoHijo": tipo_actual,
            "Titulo_proyecto": titulo,
            "a√±o": anio
        })
    print(f"‚úÖ Total CONSULTOR√çAS: {len(resultados)}")
    return resultados


#================================================
# EXTRAER EVENTOS CIENT√çFICOS
#================================================
def extraer_eventos(soup):
    

    resultados = []

    anchor = soup.find("a", {"name": "evento"})
    if not anchor:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n de eventos")
        return resultados

    contenedor = anchor.find_parent("td")

    # üîπ Cada evento empieza con un <b> num√©rico (1,2,3...)
    for b in contenedor.find_all("b"):

        if not b.get_text(strip=True).isdigit():
            continue

        td_evento = b.find_parent("td")
        if not td_evento:
            continue

        texto = limpiar(td_evento.get_text(" "))

        # üü¢ Nombre del evento
        nombre_match = re.search(
            r"Nombre del evento:\s*(.*?)(?=Tipo de evento:|√Åmbito:|Realizado el:)",
            texto,
            re.IGNORECASE
        )
        nombre_evento = nombre_match.group(1).strip() if nombre_match else ""

        # üü¢ A√±o
        anio_match = re.search(r"\b(19|20)\d{2}\b", texto)
        anio = anio_match.group() if anio_match else ""

        if nombre_evento:
            resultados.append({
                "NodoHijo": "Evento cient√≠fico",
                "Titulo_proyecto": nombre_evento,
                "a√±o": anio
            })
    print(f"‚úÖ Total EVENTOS CIENT√çFICOS: {len(resultados)}")
    return resultados

#================================================
# EXTRAER FORTALECIMIENTO O SOLUCI√ìN DE ASUNTOS DE INTER√âS
#================================================
def extraer_apropiacion_social(soup):
    

    resultados = []

    # 1Ô∏è‚É£ Buscar la secci√≥n
    h3 = soup.find("h3", string=re.compile(
        r"Fortalecimiento o soluci√≥n de asuntos de inter√©s social",
        re.IGNORECASE
    ))

    if not h3:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n de Apropiaci√≥n Social")
        return resultados

    # 2Ô∏è‚É£ Buscar todos los <b> despu√©s del h3 hasta otro h3
    for elem in h3.find_all_next():

        # üõë cortar si empieza otra secci√≥n
        if elem.name == "h3":
            break

        # üéØ detectar cada nodo hijo (<b>)
        if elem.name == "b" and "Apropiaci√≥n social del conocimiento" in elem.get_text():

            texto_b = limpiar(elem.get_text())

            # üîπ NodoHijo despu√©s del guion
            nodo_hijo = ""
            if "-" in texto_b:
                nodo_hijo = texto_b.split("-", 1)[1].strip()

            # üîπ Buscar el blockquote siguiente (producto)
            blockquote = elem.find_next("blockquote")
            if not blockquote:
                continue

            titulo = ""
            anio = ""

            children = list(blockquote.children)

            for i, child in enumerate(children):

                # Nombre del producto
                if getattr(child, "name", None) == "i" and "Nombre del producto" in child.get_text():
                    if i + 1 < len(children):
                        titulo = limpiar(children[i + 1])

                # A√±o
                if getattr(child, "name", None) == "i" and "Fecha de presentaci√≥n" in child.get_text():
                    if i + 1 < len(children):
                        texto_fecha = limpiar(children[i + 1])
                        anio_match = re.search(r"\b(19|20)\d{2}\b", texto_fecha)
                        if anio_match:
                            anio = anio_match.group()

            if titulo:
                resultados.append({
                    "NodoHijo": nodo_hijo,
                    "Titulo_producto": titulo,
                    "a√±o": anio
                })
    print(f"‚úÖ Total APROPIACI√ìN SOCIAL: {len(resultados)}")
    return resultados

#================================================
# EXTRAER GENERACI√ìN DE INSUMOS DE POL√çTICA P√öBLICA Y NORMATIVIDAD
#================================================
def extraer_apropiacion_normatividad(soup):
    

    resultados = []

    # 1Ô∏è‚É£ Buscar la secci√≥n
    h3 = soup.find("h3", string=re.compile(
        r"Generaci√≥n de insumos de pol√≠tica p√∫blica y normatividad",
        re.IGNORECASE
    ))

    if not h3:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n de Generaci√≥n de insumos de pol√≠tica p√∫blica y normatividad")
        return resultados

    # 2Ô∏è‚É£ Buscar todos los <b> despu√©s del h3 hasta otro h3
    for elem in h3.find_all_next():

        # üõë cortar si empieza otra secci√≥n
        if elem.name == "h3":
            break

        # üéØ detectar cada nodo hijo (<b>)
        if elem.name == "b" and "Apropiaci√≥n social del conocimiento" in elem.get_text():

            texto_b = limpiar(elem.get_text())

            # üîπ NodoHijo despu√©s del guion
            nodo_hijo = ""
            if "-" in texto_b:
                nodo_hijo = texto_b.split("-", 1)[1].strip()

            # üîπ Buscar el blockquote siguiente (producto)
            blockquote = elem.find_next("blockquote")
            if not blockquote:
                continue

            titulo = ""
            anio = ""

            children = list(blockquote.children)

            for i, child in enumerate(children):

                # Nombre del producto
                if getattr(child, "name", None) == "i" and "Nombre del producto" in child.get_text():
                    if i + 1 < len(children):
                        titulo = limpiar(children[i + 1])

                # A√±o
                if getattr(child, "name", None) == "i" and "Fecha de presentaci√≥n" in child.get_text():
                    if i + 1 < len(children):
                        texto_fecha = limpiar(children[i + 1])
                        anio_match = re.search(r"\b(19|20)\d{2}\b", texto_fecha)
                        if anio_match:
                            anio = anio_match.group()

            if titulo:
                resultados.append({
                    "NodoHijo": nodo_hijo,
                    "Titulo_producto": titulo,
                    "a√±o": anio
                })
    print(f"‚úÖ Total APROPIACI√ìN NORMATIVIDAD: {len(resultados)}")
    return resultados

def extraer_apropiacion_cadenas_productivas(soup):
    

    resultados = []

    # 1Ô∏è‚É£ Buscar la secci√≥n
    h3 = soup.find("h3", string=re.compile(
        r"Fortalecimiento de cadenas productivas",
        re.IGNORECASE
    ))

    if not h3:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n de Fortalecimiento de cadenas productivas")
        return resultados

    # 2Ô∏è‚É£ Buscar todos los <b> despu√©s del h3 hasta otro h3
    for elem in h3.find_all_next():

        # üõë cortar si empieza otra secci√≥n
        if elem.name == "h3":
            break

        # üéØ detectar cada nodo hijo (<b>)
        if elem.name == "b" and "Apropiaci√≥n social del conocimiento" in elem.get_text():

            texto_b = limpiar(elem.get_text())

            # üîπ NodoHijo despu√©s del guion
            nodo_hijo = ""
            if "-" in texto_b:
                nodo_hijo = texto_b.split("-", 1)[1].strip()

            # üîπ Buscar el blockquote siguiente (producto)
            blockquote = elem.find_next("blockquote")
            if not blockquote:
                continue

            titulo = ""
            anio = ""

            children = list(blockquote.children)

            for i, child in enumerate(children):

                # Nombre del producto
                if getattr(child, "name", None) == "i" and "Nombre del producto" in child.get_text():
                    if i + 1 < len(children):
                        titulo = limpiar(children[i + 1])

                # A√±o
                if getattr(child, "name", None) == "i" and "Fecha de presentaci√≥n" in child.get_text():
                    if i + 1 < len(children):
                        texto_fecha = limpiar(children[i + 1])
                        anio_match = re.search(r"\b(19|20)\d{2}\b", texto_fecha)
                        if anio_match:
                            anio = anio_match.group()

            if titulo:
                resultados.append({
                    "NodoHijo": nodo_hijo,
                    "Titulo_producto": titulo,
                    "a√±o": anio
                })
    print(f"‚úÖ Total FORTALECIMIENTO DE CADENAS PRODUCTIVAS: {len(resultados)}")
    return resultados

def extraer_produccion_contenido_transmedia(soup):
    

    resultados = []

    # 1Ô∏è‚É£ Buscar la secci√≥n
    h3 = soup.find(
        "h3",
        string=re.compile(
            r"Producci√≥n de estrategias y contenidos transmedia",
            re.IGNORECASE
        )
    )

    if not h3:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n de Producci√≥n de estrategias y contenidos transmedia")
        return resultados

    # 2Ô∏è‚É£ Recorrer elementos hasta otro h3
    for elem in h3.find_all_next():

        if elem.name == "h3":
            break

        # 3Ô∏è‚É£ Detectar el <b> correcto
        if elem.name == "b" and "producci√≥n de estrategias y contenidos transmedia" in elem.get_text().lower():

            texto_b = limpiar(elem.get_text())

            # üîπ NodoHijo = despu√©s del guion
            nodo_hijo = ""
            if "-" in texto_b:
                nodo_hijo = texto_b.split("-", 1)[1].strip()

            # 4Ô∏è‚É£ Buscar el blockquote siguiente
            blockquote = elem.find_next("blockquote")
            if not blockquote:
                continue

            titulo = ""
            anio = ""

            # 5Ô∏è‚É£ Recorrer los <i> del blockquote
            for i_tag in blockquote.find_all("i"):

                texto_i = limpiar(i_tag.get_text()).lower()

                # üîπ Nombre del producto
                if "nombre del producto" in texto_i:
                    siguiente = i_tag.next_sibling
                    if siguiente:
                        titulo = limpiar(str(siguiente))

                # üîπ A√±o
                if "fecha de presentaci√≥n" in texto_i:
                    siguiente = i_tag.next_sibling
                    if siguiente:
                        a√±o_match = re.search(r"\b(19|20)\d{2}\b", str(siguiente))
                        if a√±o_match:
                            anio = a√±o_match.group()

            if titulo:
                resultados.append({
                    "NodoHijo": nodo_hijo,
                    "Titulo_producto": titulo,
                    "a√±o": anio
                })
    print(f"‚úÖ Total PRODUCCI√ìN DE ESTRATEGIAS Y CONTENIDOS TRANSMEDIA: {len(resultados)}")
    return resultados


#================================================
# EXTRAER DESARROLLOS WEB
#================================================

def extraer_desarrollos_web(soup):
    

    resultados = []

    # 1Ô∏è‚É£ Buscar la secci√≥n
    h3 = soup.find("h3", string=re.compile(
        r"Desarrollos web",
        re.IGNORECASE
    ))

    if not h3:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n Desarrollos web")
        return resultados

    # 2Ô∏è‚É£ Recorrer elementos hasta otro h3
    for elem in h3.find_all_next():

        if elem.name == "h3":
            break

        # üéØ Detectar el <b>
        if elem.name == "b" and "Divulgaci√≥n p√∫blica de la ciencia" in elem.get_text():

            texto_b = limpiar(elem.get_text())

            nodo_hijo = ""

            # üîπ Tomar texto despu√©s del guion
            if "-" in texto_b:
                parte = texto_b.split("-", 1)[1].strip()

                # üîπ Cortar antes de los dos puntos
                if ":" in parte:
                    nodo_hijo = parte.split(":", 1)[0].strip()
                else:
                    nodo_hijo = parte.strip()

            # üîπ Buscar blockquote
            blockquote = elem.find_next("blockquote")
            if not blockquote:
                continue

            titulo = ""
            anio = ""

            children = list(blockquote.children)

            for i, child in enumerate(children):

                # Nombre del producto
                if getattr(child, "name", None) == "i" and "Nombre del producto" in child.get_text():
                    if i + 1 < len(children):
                        titulo = limpiar(children[i + 1])

                # A√±o
                if getattr(child, "name", None) == "i" and "Fecha de presentaci√≥n" in child.get_text():
                    if i + 1 < len(children):
                        texto_fecha = limpiar(children[i + 1])
                        anio_match = re.search(r"\b(19|20)\d{2}\b", texto_fecha)
                        if anio_match:
                            anio = anio_match.group()

            if titulo:
                resultados.append({
                    "NodoHijo": nodo_hijo,
                    "Titulo_producto": titulo,
                    "a√±o": anio
                })
    print(f"‚úÖ Total DESARROLLOS WEB: {len(resultados)}")
    return resultados

#================================================
# EXTRAER ART√çCULOS
#================================================
def extraer_articulos(soup):
    

    resultados = []

    # 1Ô∏è‚É£ Buscar ancla de art√≠culos
    anchor = soup.find("a", {"name": "articulos"})
    if not anchor:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n de art√≠culos")
        return resultados

    contenedor = anchor.find_parent("td")

    # 2Ô∏è‚É£ Iterar cada blockquote (cada art√≠culo o grupo de art√≠culos)
    for block in contenedor.find_all("blockquote", recursive=True):
        texto = limpiar(block.get_text(" "))

        # üü¢ Extraer todos los t√≠tulos entre comillas simples o dobles
        # Esto captura: "T√≠tulo" o ""T√≠tulo""
        titulos = re.findall(r'"{1,2}\s*(.*?)\s*"{1,2}', texto)

        # Extraemos el a√±o
        parte_antes_doi = texto.split("DOI")[0]
        anios = re.findall(r"\b(?:19|20)\d{2}\b", parte_antes_doi)
        anio = anios[-1] if anios else ""

        # Agregar todos los t√≠tulos encontrados
        for titulo in titulos:
            if titulo.strip():  # Evitar t√≠tulos vac√≠os
                resultados.append({
                    "NodoHijo": "Art√≠culo",
                    "Titulo_proyecto": titulo.strip(),
                    "a√±o": anio
                })
    print(f"‚úÖ Total ART√çCULOS: {len(resultados)}")
    return resultados

#================================================
# EXTRAER LIBROS
#================================================
def extraer_libros(soup):
    resultados = []

    # 1Ô∏è‚É£ Buscar el h3 que diga exactamente "Libros"
    h3_libros = soup.find("h3", string=re.compile(r"^Libros$", re.I))
    if not h3_libros:
        return resultados

    # 2Ô∏è‚É£ Subir a la tabla que contiene esa secci√≥n
    tabla_libros = h3_libros.find_parent("table")
    if not tabla_libros:
        return resultados

    # 3Ô∏è‚É£ Buscar todos los <li> dentro de esa tabla
    items = tabla_libros.find_all("li")

    for li in items:
        b_tag = li.find("b")
        if not b_tag:
            continue

        texto_categoria = b_tag.get_text(" ", strip=True)

        # Validar estructura tipo: Producci√≥n bibliogr√°fica - Libro - ...
        if texto_categoria.count("-") < 2:
            continue

        partes = [p.strip() for p in texto_categoria.split("-")]
        nodo_hijo = partes[1]

        # 4Ô∏è‚É£ Buscar el blockquote siguiente
        block = li.find_next("blockquote")

        # Asegurar que el blockquote pertenece a esta tabla
        if not block or block.find_parent("table") != tabla_libros:
            continue

        texto_block = block.get_text(" ", strip=True)

        # 5Ô∏è‚É£ Extraer t√≠tulo entre comillas
        match_titulo = re.search(r'"([^"]+)"', texto_block)
        if not match_titulo:
            continue

        titulo = match_titulo.group(1).strip()

        # 6Ô∏è‚É£ Extraer a√±o
        match_anio = re.search(r'\b(19|20)\d{2}\b', texto_block)
        anio = match_anio.group(0) if match_anio else None

        resultados.append({
            "NodoHijo": nodo_hijo,
            "Titulo_proyecto": titulo,
            "a√±o": anio
        })
    print(f"‚úÖ Total LIBROS: {len(resultados)}")
    return resultados


#================================================
# EXTRAER CAP√çTULOS DE LIBRO
#================================================
def extraer_capitulos_libro(soup):
    

    resultados = []

    # 1Ô∏è‚É£ Buscar el ancla
    anchor = soup.find("a", {"name": "capitulos"})
    if not anchor:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n de cap√≠tulos de libro")
        return resultados

    contenedor = anchor.find_parent("td")

    nodo_hijo = "Cap√≠tulos de libro"

    # 2Ô∏è‚É£ Cada cap√≠tulo est√° en un <blockquote>
    for block in contenedor.find_all("blockquote", recursive=True):

        texto = limpiar(block.get_text(" "))

        # üü¢ T√≠tulo (entre comillas)
        titulo_match = re.search(
            r"\"(.*?)\"",
            texto
        )
        titulo = titulo_match.group(1).strip() if titulo_match else ""

        # üü¢ A√±o
        anio_match = re.search(r"\b(19|20)\d{2}\b", texto)
        anio = anio_match.group() if anio_match else ""

        if titulo:
            resultados.append({
                "NodoHijo": nodo_hijo,
                "Titulo_proyecto": titulo,
                "a√±o": anio
            })
    print(f"‚úÖ Total CAP√çTULOS DE LIBRO: {len(resultados)}")
    return resultados

#================================================
# EXTRAER INNOVACIONES DE GESTI√ìN EMPRESARIAL
#================================================
def extraer_innovaciones_gestion_empresarial(soup):
    

    resultados = []

    # 1Ô∏è‚É£ Buscar todos los <b> que indiquen la secci√≥n
    for b in soup.find_all("b"):
        if "Producci√≥n t√©cnica - Innovaciones generadas de producci√≥n empresarial" in b.get_text():
            # NodoHijo: lo que est√° despu√©s del primer guion
            nodo_hijo = b.get_text().split("-", 1)[1].strip()

            # 2Ô∏è‚É£ Buscar el blockquote siguiente
            block = b.find_parent("td").find_next("blockquote")
            if not block:
                continue

            texto = limpiar(block.get_text(" "))

            # 3Ô∏è‚É£ T√≠tulo: todo hasta "Nombre comercial"
            hasta_nombre = re.search(r"^(.*?)(?=Nombre comercial)", texto, re.IGNORECASE)
            titulo = ""
            if hasta_nombre:
                texto_hasta_nombre = hasta_nombre.group(1).strip()

                # Buscar la √∫ltima coma que tenga may√∫scula a la derecha
                match_coma = list(re.finditer(r",\s*(?=[A-Z])", texto_hasta_nombre))
                if match_coma:
                    ultima_coma = match_coma[-1].end()
                    titulo = texto_hasta_nombre[ultima_coma:].strip(" ,")
                else:
                    titulo = texto_hasta_nombre.strip(" ,")

            # 4Ô∏è‚É£ A√±o: buscar "En: <pa√≠s>" y luego el a√±o, ignorando comas y espacios extra
            anio_match = re.search(r"En:\s*[A-Za-z\s]+(?:,\s*)*,\s*(\d{4})", texto)
            anio = anio_match.group(1) if anio_match else ""

            resultados.append({
                "NodoHijo": nodo_hijo,
                "Titulo_proyecto": titulo,
                "a√±o": anio
            })
    print(f"‚úÖ Total INNOVACIONES DE GESTI√ìN EMPRESARIAL: {len(resultados)}")
    return resultados
#================================================
# EXTRAER DOCUMENTOS DE TRABAJO
#================================================
def extraer_documentos_trabajo(soup):
    

    resultados = []

    # 1Ô∏è‚É£ Buscar secci√≥n "Documentos de trabajo"
    h3 = soup.find("h3", string=re.compile(r"Documentos de trabajo", re.IGNORECASE))
    if not h3:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n Documentos de trabajo")
        return resultados

    # 2Ô∏è‚É£ Recorrer todos los blockquote dentro de la secci√≥n hasta otro h3
    for blockquote in h3.find_all_next("blockquote"):
        if blockquote.find_previous("h3") != h3:
            break  # sali√≥ de la secci√≥n

        texto_block = limpiar(blockquote.get_text(" "))

        # 3Ô∏è‚É£ NodoHijo: buscar el <b> m√°s cercano antes del blockquote
        b_antes = blockquote.find_previous("b")
        nodo_hijo = ""
        if b_antes:
            texto_b = limpiar(b_antes.get_text())
            if "-" in texto_b:
                nodo_hijo = texto_b.split("-", 1)[1].strip()
                nodo_hijo = re.sub(r"\(.*?\)", "", nodo_hijo).strip()

        # 4Ô∏è‚É£ T√≠tulo entre comillas
        titulo_match = re.search(r'"([^"]+)"', texto_block)
        if not titulo_match:
            continue  # ignorar blockquote sin t√≠tulo
        titulo = titulo_match.group(1).strip()

        # 5Ô∏è‚É£ A√±o: primero En: <pa√≠s>, si no fallback a cualquier a√±o
        anio_match = re.search(r"En:\s*[A-Za-z\s]+(?:,\s*)*,\s*(\d{4})", texto_block)
        if anio_match:
            anio = anio_match.group(1)
        else:
            anio_match = re.search(r"\b(19|20)\d{2}\b", texto_block)
            anio = anio_match.group() if anio_match else ""

        # 6Ô∏è‚É£ Evitar duplicados: comparar NodoHijo + t√≠tulo + a√±o
        if not any(r["NodoHijo"] == nodo_hijo and r["Titulo_documento"] == titulo and r["a√±o"] == anio for r in resultados):
            resultados.append({
                "NodoHijo": nodo_hijo,
                "Titulo_documento": titulo,
                "a√±o": anio
            })
    
    print(f"‚úÖ Total DOCUMENTOS DE TRABAJO: {len(resultados)}")
    return resultados

#================================================
# EXTRAER PATENTES
#================================================
def extraer_patentes(soup):
    

    resultados = []

    anchor = soup.find("a", {"name": "patentes"})
    if not anchor:
        print("No se encontr√≥ la secci√≥n Patentes")
        return resultados

    contenedor = anchor.find_parent("td")

    nodo_hijo = "Patente"

    for blockquote in contenedor.find_all("blockquote"):

        texto = limpiar(blockquote.get_text(" "))

        # üîπ T√çTULO: despu√©s del "-" hasta la primera coma
        titulo = ""
        titulo_match = re.search(r"-\s*([^,]+)", texto)
        if titulo_match:
            titulo = titulo_match.group(1).strip()

        # üîπ A√ëO: desde fecha YYYY-MM-DD
        anio = ""
        anio_match = re.search(r"\b(19|20)\d{2}(?=-\d{2}-\d{2})", texto)
        if anio_match:
            anio = anio_match.group(0)

        if titulo:
            resultados.append({
                "NodoHijo": nodo_hijo,
                "Titulo_patente": titulo,
                "a√±o": anio
            })

    print(f"‚úÖ Total PATENTES: {len(resultados)}")
    return resultados

#================================================
# EXTRAER SECRETOS EMPRESARIALES
#================================================
def extraer_secretos_empresariales(soup ):
    

    resultados = []

    anchor = soup.find("a", {"name": "secretos"})
    if not anchor:
        print("No se encontr√≥ la secci√≥n Secretos empresariales")
        return resultados

    contenedor = anchor.find_parent("td")

    nodo_hijo = "Secreto empresarial"

    # Recorremos SOLO los <b> dentro del contenedor
    for b in contenedor.find_all("b"):

        titulo = limpiar(b.get_text())

        if titulo:
            resultados.append({
                "NodoHijo": nodo_hijo,
                "Titulo_secreto": titulo,
                "a√±o": ""
            })
    print(f"‚úÖ Total SECRETOS EMPRESARIALES: {len(resultados)}")
    return resultados

#================================================
# EXTRAER SOFTWARE
#================================================
def extraer_software(soup):
    

    resultados = []

    anchor = soup.find("a", {"name": "software"})
    if not anchor:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n de software")
        return resultados

    contenedor = anchor.find_parent("td")
    nodo_hijo = "Software"

    for block in contenedor.find_all("blockquote", recursive=True):

        texto = limpiar(block.get_text(" "))

        # ‚úÖ EXTRAER T√çTULO: lo que est√° antes de ", Nombre comercial"
        titulo_match = re.search(
            r"([^,]+)(?=,\s*Nombre comercial)",
            texto,
            re.IGNORECASE
        )
        titulo = titulo_match.group(1).strip() if titulo_match else ""

        # ‚úÖ EXTRAER A√ëO
        anio_match = re.search(r"\b(19|20)\d{2}\b", texto)
        anio = anio_match.group() if anio_match else ""

        if titulo:
            resultados.append({
                "NodoHijo": nodo_hijo,
                "Titulo_proyecto": titulo,
                "a√±o": anio
            })
    print(f"‚úÖ Total SOFTWARE: {len(resultados)}")
    return resultados


#================================================
# EXTRAER PROTOTIPOS INDUSTRIALES
#================================================
def extraer_prototipos_industriales(soup):
    
    resultados = []

    # 1Ô∏è‚É£ Buscar la secci√≥n Prototipos
    h3 = soup.find("h3", string=re.compile(r"Prototipos", re.IGNORECASE))
    if not h3:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n Prototipos")
        return resultados

    # 2Ô∏è‚É£ Contenedor general
    contenedor = h3.find_parent("table")

    nodo_hijo = "Prototipo industrial"

    # 3Ô∏è‚É£ Recorremos todos los <b> de prototipo industrial
    for b in contenedor.find_all("b"):

        texto_b = limpiar(b.get_text())

        if "Prototipo - Industrial" not in texto_b:
            continue

        # 4Ô∏è‚É£ El blockquote SIEMPRE est√° en el siguiente <tr>
        tr = b.find_parent("tr")
        siguiente_tr = tr.find_next_sibling("tr")
        if not siguiente_tr:
            continue

        blockquote = siguiente_tr.find("blockquote")
        if not blockquote:
            continue

        texto = blockquote.get_text(" ", strip=True)

        # 5Ô∏è‚É£ T√çTULO ‚Üí antes de "Nombre comercial:"
        parte_util = texto.split("Nombre comercial:")[0]
        fragmentos = [f.strip() for f in parte_util.split(",") if f.strip()]
        titulo = fragmentos[-1] if fragmentos else ""

        # 6Ô∏è‚É£ A√ëO
        anio_match = re.search(r",\s*(19|20)\d{2}\s*,", texto)
        anio = anio_match.group(0).replace(",", "").strip() if anio_match else ""

        if titulo:
            resultados.append({
                "NodoHijo": nodo_hijo,
                "Titulo_prototipo": limpiar(titulo),
                "a√±o": anio
            })
    print(f"‚úÖ Total PROTOTIPOS INDUSTRIALES: {len(resultados)}")
    return resultados

#================================================
# EXTRAER INNOVACI√ìN DE PROCESO O PROCEDIMIENTO
#================================================

def extraer_innovacion_procesos(soup):
    

    resultados = []

    # 1Ô∏è‚É£ Buscar el h3 exacto
    h3 = soup.find("h3", string=re.compile(
        r"Innovaci√≥n de proceso o procedimiento", re.IGNORECASE
    ))

    if not h3:
        print("‚ö†Ô∏è No se encontr√≥ el h3 de Innovaci√≥n de proceso o procedimiento")
        return resultados

    # 2Ô∏è‚É£ Recorrer hasta el siguiente h3
    for elem in h3.find_all_next():

        if elem.name == "h3":
            break

        if elem.name != "blockquote":
            continue

        texto = limpiar(elem.get_text(" "))

        # ‚úÖ T√çTULO: texto antes de ", Nombre comercial"
        titulo_match = re.search(
            r"([^,]+)(?=,\s*Nombre comercial)",
            texto,
            re.IGNORECASE
        )
        titulo = titulo_match.group(1).strip() if titulo_match else ""

        # ‚úÖ A√ëO
        anio_match = re.search(r"\b(19|20)\d{2}\b", texto)
        anio = anio_match.group() if anio_match else ""

        if titulo:
            resultados.append({
                "NodoHijo": "Innovaci√≥n de proceso o procedimiento",
                "Titulo_proyecto": titulo,
                "a√±o": anio
            })
    print(f"‚úÖ Total INNOVACI√ìN DE PROCESOS O PROCEDIMIENTOS: {len(resultados)}")
    return resultados

#================================================
# EXTRAER INFORMES T√âCNICOS
#================================================

def extraer_informes_tecnicos(soup):

    resultados = []
    nodo_hijo = "Informe t√©cnico"

    # 1Ô∏è‚É£ Buscar el comentario que marca el fin del bloque anterior
    comentario = soup.find(
        string=lambda text: isinstance(text, str) and "Fin Nuevo registro cientifico" in text
    )

    if not comentario:
        print("‚ö†Ô∏è No se encontr√≥ el comentario de referencia")
        return resultados

    # 2Ô∏è‚É£ Desde ah√≠ buscar el h3 correcto
    seccion = comentario.find_next("h3", id="trabajos_tec")

    if not seccion:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n trabajos_tec")
        return resultados

    # 3Ô∏è‚É£ Subir a la tabla contenedora correcta
    tabla = seccion.find_parent("table")

    if not tabla:
        print("‚ö†Ô∏è No se encontr√≥ tabla contenedora")
        return resultados

    # 4Ô∏è‚É£ Buscar SOLO los blockquote dentro de esa tabla
    bloques = tabla.find_all("blockquote")

    for block in bloques:

        # Obtener texto con saltos de l√≠nea reales
        texto = block.get_text("\n", strip=True)

        # üîπ Extraer a√±o (√∫ltimo a√±o de 4 d√≠gitos)
        anios = re.findall(r"\b(?:19|20)\d{2}\b", texto)
        anio = anios[-1] if anios else ""

        # üîπ Tomar solo la parte antes de "Nombre comercial"
        parte_principal = texto.split("Nombre comercial")[0]

        # üîπ Separar l√≠neas limpias
        lineas = [l.strip(" ,") for l in parte_principal.split("\n") if l.strip()]

        # üîπ El t√≠tulo suele ser la √∫ltima l√≠nea antes de "Nombre comercial"
        titulo = lineas[-1] if lineas else ""
        titulo = quitar_tildes(titulo)

        resultados.append({
            "NodoHijo": nodo_hijo,
            "Titulo_proyecto": titulo,
            "a√±o": anio
        })

    print(f"‚úÖ Total informes t√©cnicos encontrados: {len(resultados)}")

    return resultados


#================================================
# EXTRAER CONCEPTOS T√âCNICOS
#================================================
def extraer_conceptos_tecnicos(soup):
    

    resultados = []

    # 1Ô∏è‚É£ Buscar todos los <b> que indiquen "Producci√≥n t√©cnica - Concepto t√©cnico"
    for b in soup.find_all("b"):
        if "Producci√≥n t√©cnica - Concepto t√©cnico" in b.get_text():
            # NodoHijo: lo que est√° despu√©s del guion
            nodo_hijo = b.get_text().split("-", 1)[1].strip()

            # 2Ô∏è‚É£ Buscar el blockquote siguiente
            block = b.find_parent("td").find_next("blockquote")
            if not block:
                continue

            texto = limpiar(block.get_text(" "))

            # 3Ô∏è‚É£ T√≠tulo: todo hasta "Instituci√≥n solicitante"
            hasta_institucion = re.search(r"^(.*?)(?=Instituci√≥n solicitante)", texto, re.IGNORECASE)
            titulo = ""
            if hasta_institucion:
                texto_hasta_inst = hasta_institucion.group(1).strip()

                # Buscar la √∫ltima coma que tenga may√∫scula a la derecha (para eliminar autores)
                match_coma = list(re.finditer(r",\s*(?=[A-Z])", texto_hasta_inst))
                if match_coma:
                    ultima_coma = match_coma[-1].end()
                    titulo = texto_hasta_inst[ultima_coma:].strip(" ,")
                else:
                    titulo = texto_hasta_inst.strip(" ,")

            # 4Ô∏è‚É£ A√±o: buscar "Fecha solicitud" o "Fecha de env√≠o" y tomar 4 d√≠gitos
            anio_match = re.search(r"Fecha solicitud:.*?(\d{4})", texto)
            if not anio_match:
                anio_match = re.search(r"Fecha de env√≠o:.*?(\d{4})", texto)
            anio = anio_match.group(1) if anio_match else ""

            resultados.append({
                "NodoHijo": nodo_hijo,
                "Titulo_proyecto": titulo,
                "a√±o": anio
            })
    print(f"‚úÖ Total CONCEPTOS T√âCNICOS: {len(resultados)}")
    return resultados




#================================================
# EXTRAER INFORMES FINALES DE INVESTIGACI√ìN
#================================================
def extraer_informes_finales_investigacion(soup):

    resultados = []
    nodo_hijo = "Informes finales de investigaci√≥n"

    # üîπ Buscar el h3 correctamente (tolerante)
    seccion = soup.find(
        "h3",
        string=lambda t: t and "Informes de investig" in t
    )

    if not seccion:
        return resultados

    tabla = seccion.find_parent("table")
    if not tabla:
        return resultados

    # üîπ Iterar cada bloque
    for block in tabla.find_all("blockquote", recursive=True):

        texto = block.get_text(" ", strip=True)

        # ========================
        # 1Ô∏è‚É£ Extraer a√±o
        # ========================
        anio_match = re.search(r"\b(19|20)\d{2}\b", texto)
        anio = anio_match.group(0) if anio_match else ""

        # ========================
        # 2Ô∏è‚É£ Extraer t√≠tulo
        # ========================

        # Cortar antes de ". En:"
        titulo_bruto = re.split(r"\.?\s*En:", texto, flags=re.IGNORECASE)[0]

        # Eliminar el autor (todo hasta la primera coma)
        if "," in titulo_bruto:
            titulo_bruto = titulo_bruto.split(",", 1)[1]

        titulo = titulo_bruto.strip(" ,.")

        # Limpieza extra
        titulo = re.sub(r"\s{2,}", " ", titulo)

        if titulo:
            resultados.append({
                "NodoHijo": nodo_hijo,
                "Titulo_proyecto": titulo,
                "a√±o": anio
            })
    print(f"‚úÖ Total INFORMES FINALES DE INVESTIGACI√ìN: {len(resultados)}")
    return resultados

#================================================
# EXTRAER PROYECTOS
#================================================
def extraer_proyectos(soup):

    resultados = []

    # 1Ô∏è‚É£ Buscar la secci√≥n Proyectos
    h3 = soup.find("h3", string=re.compile(r"Proyectos", re.IGNORECASE))
    if not h3:
        print("‚ö†Ô∏è No se encontr√≥ la secci√≥n Proyectos")
        return resultados

    # 2Ô∏è‚É£ Recorrer hasta otro h3
    for elem in h3.find_all_next():

        if elem.name == "h3":
            break

        if elem.name != "blockquote":
            continue

        nodo_hijo = ""
        titulo = ""
        anio = ""

        children = list(elem.children)

        for i, child in enumerate(children):

            # üîπ Detectar <i> Tipo de proyecto
            if getattr(child, "name", None) == "i" and "Tipo de proyecto" in child.get_text():

                # üëâ El valor REAL est√° en el siguiente nodo
                if i + 1 < len(children):
                    nodo_hijo = limpiar(children[i + 1])
                    nodo_hijo = nodo_hijo.replace(",", "")

            # üîπ Texto plano
            if isinstance(child, str):
                texto = limpiar(child)

                if not texto:
                    continue

                # ‚úÖ T√çTULO (primer texto largo que NO sea el tipo)
                if not titulo and texto != nodo_hijo and len(texto) > 5:
                    titulo = texto
                    titulo = limpiar_titulo(titulo)

                # ‚úÖ A√ëO
                anio_match = re.search(r"\b(19|20)\d{2}\b", texto)
                if anio_match:
                    anio = anio_match.group()

        if titulo:
            resultados.append({
                "NodoHijo": nodo_hijo,
                "Titulo_proyecto": titulo,
                "a√±o": anio
            })
    print(f"‚úÖ Total PROYECTOS: {len(resultados)}")
    return resultados



def guardar_csv(filas):
    archivo = "cv_datos_generales.csv"
    existe = os.path.exists(archivo)

    with open(archivo, "a", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(
            f,
            fieldnames=[
                "categoria",
                "nombre",
                "sexo",
                "UltimaFormacionAcademica",
                "NodoHijo",
                "Titulo_proyecto",
                "a√±o"
            ]
        )

        # üîπ Escribir encabezado SOLO si el archivo no existe
        if not existe:
            writer.writeheader()

        writer.writerows(filas)

def main():
    print("Iniciando scraping CVLAC...")

    html = obtener_html()
    with open("debug.html", "w", encoding="utf-8") as f:
        f.write(html)

    soup = BeautifulSoup(html, "lxml")

    # -----------------------------
    # Inicializar variables
    # -----------------------------
    filas_csv = []  # Para guardar todo antes de exportar CSV
    filas_mysql = []  # Para la base de datos

    # -----------------------------
    # Extraer secciones
    # -----------------------------
    datos_generales = extraer_datos_generales(soup)
    extra_formacion = extraer_ultima_formacion_academica(soup)
    trabajos = extraer_trabajos_dirigidos(soup)
    consultorias = extraer_consultorias(soup)
    eventos = extraer_eventos(soup)
    apropiacion_social = extraer_apropiacion_social(soup)
    apropiacion_normatividad = extraer_apropiacion_normatividad(soup)
    cadenas_productivas = extraer_apropiacion_cadenas_productivas(soup)
    contenido_transmedia = extraer_produccion_contenido_transmedia(soup)
    desarrollos_web = extraer_desarrollos_web(soup)
    articulos = extraer_articulos(soup)
    libros = extraer_libros(soup)
    capitulos_libro = extraer_capitulos_libro(soup)
    innovaciones_gestion_empresarial = extraer_innovaciones_gestion_empresarial(soup)
    documentos_trabajo = extraer_documentos_trabajo(soup)
    patentes = extraer_patentes(soup)
    secretos_empresariales = extraer_secretos_empresariales(soup)
    software = extraer_software(soup)
    prototipos_industriales = extraer_prototipos_industriales(soup)
    innovacion_procesos = extraer_innovacion_procesos(soup)
    informes_tecnicos = extraer_informes_tecnicos(soup)
    conceptos_tecnicos = extraer_conceptos_tecnicos(soup)
    informes_finales_investigacion = extraer_informes_finales_investigacion(soup)
    proyectos = extraer_proyectos(soup)

    # -----------------------------
    # Construir filas_csv
    # -----------------------------
    secciones = [
        (trabajos, "Titulo_proyecto"),
        (consultorias, "Titulo_proyecto"),
        (eventos, "Titulo_proyecto"),
        (apropiacion_social, "Titulo_producto"),
        (apropiacion_normatividad, "Titulo_producto"),
        (cadenas_productivas, "Titulo_producto"),
        (contenido_transmedia, "Titulo_producto"),
        (desarrollos_web, "Titulo_producto"),
        (articulos, "Titulo_proyecto"),
        (libros, "Titulo_proyecto"),
        (capitulos_libro, "Titulo_proyecto"),
        (innovaciones_gestion_empresarial, "Titulo_proyecto"),
        (documentos_trabajo, "Titulo_documento"),
        (patentes, "Titulo_patente"),
        (secretos_empresariales, "Titulo_secreto"),
        (software, "Titulo_proyecto"),
        (prototipos_industriales, "Titulo_prototipo"),
        (innovacion_procesos, "Titulo_proyecto"),
        (informes_tecnicos, "Titulo_proyecto"),
        (conceptos_tecnicos, "Titulo_proyecto"),
        (informes_finales_investigacion, "Titulo_proyecto"),
        (proyectos, "Titulo_proyecto"),
    ]

    for seccion, campo_titulo in secciones:
        for item in seccion:
            filas_csv.append({
                "categoria": datos_generales.get("categoria", ""),
                "nombre": datos_generales.get("nombre", ""),
                "sexo": datos_generales.get("sexo", ""),
                "UltimaFormacionAcademica": extra_formacion.get("UltimaFormacionAcademica", ""),
                "NodoHijo": item.get("NodoHijo", ""),
                "Titulo_proyecto": item.get(campo_titulo, ""),
                "a√±o": item.get("a√±o", "")
            })

    # -----------------------------
    # Guardar CSV
    # -----------------------------
    guardar_csv(filas_csv)
    print(f"‚úì {len(filas_csv)} registros guardados en cvlac_completo.csv")

    # -----------------------------
    # Preparar filas para MySQL y guardar
    # -----------------------------
    for fila in filas_csv:
        filas_mysql.append({
            "categoria": fila["categoria"],
            "nombre": fila["nombre"],
            "sexo": fila["sexo"],
            "grado": fila["UltimaFormacionAcademica"],
            "tipo_proyecto": fila["NodoHijo"],
            "titulo_proyecto": fila["Titulo_proyecto"],
            "anio": fila["a√±o"]
        })
    guardar_en_mysql(filas_mysql)
    print(f"‚úì {len(filas_mysql)} registros guardados en MySQL")

if __name__ == "__main__":
    URLS = [
        "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0001671425",
        "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0001685519",
        "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0000113761",
        "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0001473049",
        "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0001740020",
        "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0000003029", #Javier Cordoba
        "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0001006690", #Jhon ni√±o
        "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0000674400&mostrar=produccion", #Walter arboleda
        "https://scienti.minciencias.gov.co/cvlac/visualizador/generarCurriculoCv.do?cod_rh=0001413648"
    ]

    for url in URLS:
        print(f"\nüìÑ Procesando CVLAC: {url}")
        URL = url     # üî• ESTA L√çNEA ES LA CLAVE
        main()